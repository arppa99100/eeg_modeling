{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "sys.path.insert(0, \"/home/anonymous/Desktop/eeg_sampling/modeling\")\n",
    "from dataio import readdata, readlabels, writedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: (105, 1130, 8) (105,)\n",
      "Validation: (21, 1130, 8) (21,)\n",
      "Testing: (26, 1130, 8) (26,)\n"
     ]
    }
   ],
   "source": [
    "#Read training\n",
    "train_dataset = readdata(\"./curated/train_dataset\")\n",
    "train_labels = readlabels(\"./curated/train_labels\")\n",
    "#Read validation\n",
    "valid_dataset = readdata(\"./curated/valid_dataset\")\n",
    "valid_labels = readlabels(\"./curated/valid_labels\")\n",
    "#Read test\n",
    "test_dataset = readdata(\"./curated/test_dataset\")\n",
    "test_labels = readlabels(\"./curated/test_labels\")\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_input = 8*1130 # EEG data input (8 channels * 1130 sample points)\n",
    "n_classes = 5 # EEG total classes (\"nothing\", \"up\", \"down\", \"left\", \"right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def onehot(data, n_classes):\n",
    "    b = np.zeros((data.size, n_classes))\n",
    "    b[np.arange(data.size), data] = 1\n",
    "    return(b)\n",
    "\n",
    "train_dataset.shape = (train_dataset.shape[0], n_input)\n",
    "valid_dataset.shape = (valid_dataset.shape[0], n_input)\n",
    "test_dataset.shape = (test_dataset.shape[0], n_input)\n",
    "train_labels = onehot(train_labels, n_classes)\n",
    "valid_labels = onehot(valid_labels, n_classes)\n",
    "test_labels = onehot(test_labels, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.0001\n",
    "training_epochs = 20\n",
    "display_step = 2\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 3000 # 1st layer num features\n",
    "n_hidden_2 = 5000 # 2nd layer num features\n",
    "n_hidden_3 = 3000 # 2nd layer num features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(\"float\", [None, n_input])\n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(_X, _weights, _biases):\n",
    "    layer_1 = tf.nn.relu(tf.add(tf.matmul(_X, _weights[\"h1\"]), _biases[\"b1\"]))\n",
    "    layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, _weights[\"h2\"]), _biases[\"b2\"]))\n",
    "    layer_3 = tf.nn.relu(tf.add(tf.matmul(layer_2, _weights[\"h3\"]), _biases[\"b3\"]))\n",
    "    return tf.matmul(layer_3, _weights[\"out\"]) + _biases[\"out\"]\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    \"h1\": tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    \"h2\": tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    \"h3\": tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "    \"out\": tf.Variable(tf.random_normal([n_hidden_3, n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    \"b1\": tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    \"b2\": tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    \"b3\": tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    \"out\": tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y)) # Softmax loss\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost) # Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.375289224\n",
      "Epoch: 0003 cost= 0.389421223\n",
      "Epoch: 0005 cost= 0.544387214\n",
      "Epoch: 0007 cost= 0.254830071\n",
      "Epoch: 0009 cost= 0.219137975\n",
      "Epoch: 0011 cost= 0.040999933\n",
      "Epoch: 0013 cost= 0.000637941\n",
      "Epoch: 0015 cost= 0.000000000\n",
      "Epoch: 0017 cost= 0.000000000\n",
      "Epoch: 0019 cost= 0.000000000\n",
      "Optimization Finished!\n",
      "Accuracy: 0.346154\n"
     ]
    }
   ],
   "source": [
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        sess.run(optimizer, feed_dict={x: train_dataset, y: train_labels})\n",
    "        # Compute average loss\n",
    "        avg_cost += sess.run(cost, feed_dict={x: train_dataset, y: train_labels})/train_dataset.size\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: test_dataset, y: test_labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
